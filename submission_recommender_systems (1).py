# -*- coding: utf-8 -*-
"""Submission_Recommender_Systems.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p-fjaV-pdAcbCaQNBm6Hx2FRXBpu02zn

# Nama : Bryant Steven Aritonang
# ID Dicoding : bryantarios

## **A. Business Understanding**

Building machine learning models for movie recommendations

## **B. Data Understanding**

### **Preparing Dataset**

Saya menggunakan perintah wget di Colaboratory untuk mengunduh dataset dari sumber eksternal. Dataset ini merupakan data sekunder yang diperoleh melalui website GroupLens dengan judul [MovieLens 25M Dataset](https://grouplens.org/datasets/movielens/#:~:text=MovieLens%2025M%20Dataset). Grup penelitian GroupLens telah mengumpulkan dan menyediakan dataset film dari situs web [MovieLens](https://movielens.org).
"""

# datasets
! wget https://files.grouplens.org/datasets/movielens/ml-25m.zip

# unzip
! unzip ml-25m.zip -d /content/data/

"""Dataset sudah ter-extract dan berada pada folder terpisah di path '/content/data/ml-25m/'

### **Data Loading**

Saya ingin melihat ada file apa saja yang terdapat pada file zip. pertama, saya akan mencari directory path dari datasetnya. kemudian menggunakan basic command 'ls' untuk menampilkan list files-nya.
"""

# current directory
! pwd

# Commented out IPython magic to ensure Python compatibility.
# change directory
# %cd data/ml-25m/

# list directory contents
! ls

"""Dataset utama yang bernama movies.csv berisi kumpulan data film yang akan diimplementasikan dalam sistem rekomendasi."""

# loads library
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# ignore all future warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""Cek jumlah data pada masing-masing dataset"""

# define dataset variable
movies = pd.read_csv('/content/data/ml-25m/movies.csv') #data film
ratings = pd.read_csv('/content/data/ml-25m/ratings.csv') #data ratings film
tags = pd.read_csv('/content/data/ml-25m/tags.csv') # data tag film
links = pd.read_csv('/content/data/ml-25m/links.csv') #data links setiap film
genome_tags = pd.read_csv('/content/data/ml-25m/genome-tags.csv') #data relevansi tag film
genome_scores = pd.read_csv('/content/data/ml-25m/genome-scores.csv') #data skor relevansi tag film

# check and count unique values in each dataframes
print('Jumlah data film: ', len(movies.movieId.unique()))
print('Jumlah data ratings atau penilaian: ', len(ratings.userId.unique()))
print('Jumlah data tags film: ', len(tags.userId.unique()))
print('Jumlah data links film: ', len(links.movieId.unique()))
print('Jumlah data genome tags: ', len(genome_tags.tagId.unique()))
print('Jumlah data genome scores: ', len(genome_scores.movieId.unique()))

"""Dataset film memiliki jumlah data yang cukup besar, yakni sekitar 62 ribu, hal ini sangat positif untuk pengembangan model rekomendasi. Informasi terperinci mengenai dataset dapat diperoleh dengan menggunakan fungsi .info()."""

# movies information
movies.info()

# ratings information
ratings.info()

# tags information
tags.info()

# links information
links.info()

# genome_tags information
genome_tags.info()

# genome_scores information
genome_scores.info()

"""Setelah mempertimbangkan informasi sebelumnya, saya berniat untuk memisahkan keenam file tersebut menjadi tiga kategori data utama, yaitu films, users, dan skor relevansi films. Selanjutnya, saya akan mengamati jumlah data yang terdapat dalam setiap kategori tersebut.






"""

# skor relevansi tag
# dataframe by tagId

# Menggabungkan seluruh tagId pada kategori genome
genome_tags_all = np.concatenate((
    genome_tags.tagId.unique(),
    genome_scores.tagId.unique()
))

# Mengurutkan data dan menghapus data yang sama
genome_tags_all = np.sort(np.unique(genome_tags_all))

print('Jumlah seluruh data genome_tags: ', len(genome_tags_all))

# ID film konsisten antara ratings.csv, tags.csv, movies.csv, dan links.csv
# (yaitu, id yang sama merujuk ke film yang sama di keempat file data ini)
# dataframe by moviesId
import numpy as np

# Menggabungkan seluruh movieId pada kategori movies
movies_all = np.concatenate((
    movies.movieId.unique(),
    tags.movieId.unique(),
    ratings.movieId.unique(),
    links.movieId.unique()
))

# Mengurutkan data dan menghapus data yang sama
movies_all = np.sort(np.unique(movies_all))

print('Jumlah seluruh data movies berdasarkan movieId: ', len(movies_all))

# ID pengguna konsisten antara ratings.csvdan tags.csv
# (yaitu, id yang sama merujuk ke pengguna yang sama di dua file)
# dataframe by userId
import numpy as np

# Menggabungkan seluruh userId pada kategori user
user_all = np.concatenate((
    ratings.userId.unique(),
    tags.userId.unique()
))

# Mengurutkan data dan menghapus data yang sama
user_all = np.sort(np.unique(user_all))

print('Jumlah seluruh data user berdasarkan userId: ', len(user_all))

"""Berdasarkan informasi tersebut, terlihat bahwa jumlah data pada relevancy score sangat terbatas, hanya sekitar 1K. Hal ini kontras dengan jumlah data pengguna dan film yang masing-masing mencapai sekitar 62K dan 162K. Oleh karena itu, keputusan saya adalah melanjutkan dengan menggunakan data film dan user saja.

## **C. Data Preparation**

### **Data Cleaning I**

**Films**
"""

# dataset films
movies

"""Untuk menghindari redundansi data, saya akan memisahkan judul film dari tahun rilisnya, sehingga informasi tersebut dapat dielaborasi dengan lebih jelas."""

#separate tittles by year of release
movies['year_of_release'] = movies.title.str.extract('([0-9]{4})')
movies.head()

"""Menghilangkan tahun rilis pada kolom judul films"""

#convert column to string
movies['title'] = movies['title'].astype(str)

#remove year
movies['title'] = movies['title'].str.split('(', 1).str[0].str.strip()
movies.head()

"""Untuk sementara, dataframe films sudah terlihat cukup baik.

**Users ratings**
"""

# dataset user ratings
ratings

"""mengecek isi data pada kolom ratings"""

# check values in rating columns
ratings.rating.unique()

"""Kolom ratings ternyata menunjukkan sebaran data yang tidak normal, dengan rentang rating dari 0.5 hingga 5 dan perbedaan skala 0.5. Untuk konsistensi, saya akan membulatkan nilai-nilai rating tersebut sehingga mencapai skala 1 hingga 5."""

# round values
ratings['rating'] = ratings['rating'].apply(np.ceil)

# recheck values in rating columns
ratings.rating.unique()

"""Saya akan mengonversi timestamp menjadi datetime. Timestamp merupakan informasi waktu yang disandikan dengan serangkaian karakter yang mengidentifikasi kapan peristiwa terjadi dengan memberikan tanggal dan waktu, yang dapat akurat hingga sepersekian detik."""

# unit='s' to convert it into epoch time
import datetime

ratings.timestamp = pd.to_datetime(ratings['timestamp'], unit='s')
ratings.head()

"""Untuk sementara, dataframe user ratings sudah terlihat cukup baik.

### **Data Merging & Cleaning II**

**Films and User ratings**

Selanjutnya adalah menggabungkan kedua dataframe sebelumnya (films and user ratings) menjadi satu dataframe yang utuh.
"""

# merge dataframe
films = pd.merge(movies, ratings, on='movieId', how='left')
films

"""Memeriksa missing value"""

# check missing values
(films.isnull() | films.empty | films.isna()).sum()

# handling missing values
films = films.dropna()
films

# recheck missing values
(films.isnull() | films.empty | films.isna()).sum()

"""Saat melakukan penanganan nilai yang hilang pada tahap sebelumnya, ditemukan data '(no genres listed)' pada kolom genre film."""

# show genre films
import sys

np.set_printoptions(threshold=sys.maxsize)
print('Banyak genre films: ', len(films['genres'].unique()))
print('Genre films: ', films['genres'].unique())
np.set_printoptions(threshold=None)

"""Memang benar, ditemukan '(no genres listed)', yang menunjukkan bahwa ada beberapa film yang tidak memiliki genre. Dalam proyek ini, akan diterapkan pendekatan content-based filtering, dengan genre film menjadi fitur yang sangat penting. Sebelum melanjutkan, saya akan mengecek seberapa banyak film yang tidak memiliki genre."""

# show non-genre
films[films['genres']=='(no genres listed)']

"""Ditemukan sebanyak 23K data film yang tidak memiliki genre. Mengingat jumlah total data yang mencapai sekitar 25 juta, saya akan menghapus data tersebut karena proporsinya relatif kecil."""

# clean non-genre
films = films[(films.genres != '(no genres listed)')]
films.head()

"""Dari total 25 juta data sebelumnya, saya menduga beberapa film mendapatkan sedikit ulasan dari pengguna. Dengan asumsi bahwa rating terendah adalah 1 dan rating tertinggi adalah 5, saya berencana untuk menghapus film-film yang mendapat penilaian kurang dari 100 kali dari pengulas, atau dengan kata lain, yang memiliki jumlah rating antara 50 hingga 250, setidaknya harus memiliki 50 rating."""

films.groupby('movieId').sum()

# remove the low rating
get_values=films['movieId'].value_counts()
temp = films.movieId.value_counts()[get_values>=50].index
films = films[films['movieId'].isin(temp)]
films

"""Cek duplikat Data"""

# duplicated by movieId
films.duplicated('movieId').sum()

# duplicated by title
films.duplicated('title').sum()

"""terdapat banyak duplikasi data didalamnya, selanjutnya saya akan menghapusnya."""

# drop duplicated data by movieId & title
films = films.drop_duplicates('movieId')
films = films.drop_duplicates('title')

"""Ketika melihat genre film sebelumnya, saya menemukan genre 'Sci-Fi'. Setelah saya teliti, ternyata 'Sci-Fi' adalah singkatan dari 'Science Fiction' yang menggambarkan film fiksi ilmiah. Kata 'Sci-Fi' menggunakan tanda pisah atau dash sebagai separator. Untuk menjaga konsistensi pada tahap vektorisasi TF-IDF, perlu menghilangkan tanda pisah ini. Jika tidak dihilangkan, pada tahap tersebut kata 'Sci-Fi' akan diperlakukan sebagai dua kata terpisah, yaitu 'Sci' dan 'Fi'."""

# replace the matching strings for 'sci-fi' using regex
films = films.replace(to_replace ='[nS]ci-Fi', value = 'Scifi', regex = True)
films.head()

"""Saya akan membuat variabel 'preparation' untuk menyimpan data yang telah dibersihkan."""

# create preparation variable
preparation = films
preparation.sort_values('movieId').head()

"""Konversi Data Series ke dalam bentuk List"""

# Convert data series ‘movieId’ to list form
film_id = preparation['movieId'].tolist()

# Convert data series ‘title’ to list form
film_name = preparation['title'].tolist()

# Convert data series ‘genres’ to list form
film_genre = preparation['genres'].tolist()

print(len(film_id))
print(len(film_name))
print(len(film_genre))

# Create dataframe using dict form of ‘film_id’, ‘film_name’, and ‘film_genre’
df_film = pd.DataFrame({
    'film_id': film_id,
    'film_name': film_name,
    'genre': film_genre
})

df_film

"""## **D. Model Development**

### **Content Based Filtering**

**Assign dataframe to new variable**

Assign Dataframe ke variabel Data
"""

# data sample
data = df_film
data.sample(5)

"""**TF-IDF Vectorizer**

Proses term frequency-inverse document frequency (TF-IDF) untuk mencari kata yang penting dalam kolom genre. setelah melakukan perhitungan idf akan didapatkan index. kemudian, saya akan mencoba melakukan mapping untuk menampilkan data genre-nya.

"""

# library
from sklearn.feature_extraction.text import TfidfVectorizer

# create object TfidfVectorizer
tf = TfidfVectorizer()

# idf
tf.fit(data['genre'])

# mapping array
tf.get_feature_names()

# fit & transform to matrix
tfidf_matrix = tf.fit_transform(data['genre'])

# show matrix dimension
tfidf_matrix.shape

"""Ukuran matriks yang dihasilkan adalah (12608, 20), dengan 12608 jumlah data dan 20 genre film. Karena hasil dari 'tfidf' masih berbentuk vektor, selanjutnya saya akan mengubahnya ke dalam bentuk matriks."""

# change tf-idf vector to matrix form
tfidf_matrix.todense()

"""melihat hasil matriks tf-idf untuk beberapa sample film"""

# dataframe tf-idf matrix, row: film_name, columns: genre_films
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names(),
    index=data.film_name
).sample(20, axis=1).sample(10, axis=0)

"""**Cosine Similarity**

Langkah berikutnya adalah menghitung kemiripan antar film, di mana setiap film akan diukur kesamaannya dengan film lainnya berdasarkan genre.
"""

# calculate cosine similarity on matrix
from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""menampilkan similarity matrix setiap film dengan menampilkan nama film dalam baris dan kolom."""

# create dataframe from the results of cosine similarity
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['film_name'], columns=data['film_name'])
print('Shape:', cosine_sim_df.shape)

# show similarity matrix
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""**Get Recommendations**

Mengembangkan sebuah fungsi yang akan memberikan rekomendasi film dengan menyajikan lima pilihan teratas.
"""

# function recommendations
def film_recommendations(film_name, similarity_data=cosine_sim_df, items=data[['film_name', 'genre']], k=5):
    # get data index
    index = similarity_data.loc[:,film_name].to_numpy().argpartition(
        range(-1, -k, -1))

    # retrieve data from an existing index
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # drop film_name you want to search
    closest = closest.drop(film_name, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""melihat data sample"""

# sample data
data.sample(3)

"""Dalam contoh data tersebut, terdapat film 'Harry Potter and the Prisoner of Azkaban'. Saya akan mencari rekomendasi film yang sesuai untuk itu."""

# check data
data[data.film_name.eq('Harry Potter and the Prisoner of Azkaban')]

"""mencari film rekomendasi untuk 'Harry Potter and the Prisoner of Azkaban'"""

# get recommendations
film_recommendations('Harry Potter and the Prisoner of Azkaban')

"""### **Collaborative Filtering**

**Data Understanding**

menyiapkan beberapa library yang dibutuhkan
"""

# loads libraries
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path

"""menyiapkan dataset 'preparation' sebelumnya dan meng-assignnya ke dalam variabel 'df'"""

# read dataset
df = preparation
df

"""**Data Preparation**

melakukan encoding pada fitur ‘userId’ dan ‘film_id’ ke dalam index
"""

# change unique values of 'userId' to list
user_ids = df['userId'].unique().tolist()
print('list userID: ', user_ids)

# encode 'userId'
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# encoding index to 'userId'
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# change unique values of 'movieId' to list
films_ids = df['movieId'].unique().tolist()

# encode 'movieId'
films_to_films_encoded = {x: i for i, x in enumerate(films_ids)}

# encoding index to 'movieId'
films_encoded_to_films = {i: x for i, x in enumerate(films_ids)}

"""mapping userId dan movieId ke dalam dataframe"""

# Mapping 'userId' to dataframe
df['user'] = df['userId'].map(user_to_user_encoded)

# Mapping 'movieId' ke dataframe
df['films'] = df['movieId'].map(films_to_films_encoded)

"""mendapatkan jumlah user, films, dan nilai user ratings"""

# get number of users
num_users = len(user_to_user_encoded)
print(num_users)

# get number of films
num_films = len(films_encoded_to_films)
print(num_films)

# change dtype
df['rating'] = df['rating'].values.astype(np.float32)

# get min values of rating
min_rating = min(df['rating'])

# get max values of rating
max_rating = max(df['rating'])

print('Number of User: {}, Number of Films: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_films, min_rating, max_rating
))

"""**Split Data for Training and Validation**

melakukan sampling dan pembagian data menjadi data training dan validasi.
"""

# sampling
df = df.sample(frac=1, random_state=42)
df

"""membagi data untuk data train dan validasi dengan komposisi 80/20."""

# mapping users and films data into one value
x = df[['user', 'films']].values

# ratings
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# split data train and validation with 80/20 composition
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""Yeay, data telah siap untuk digunakan dalam proses training model.

**Proses Training**

mengembangkan model untuk menghitung skor kecocokan antara users dan films menggunakan teknik embedding
"""

# class recommendations
class RecommenderNet(tf.keras.Model):

  # __init__
  def __init__(self, num_users, num_films, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_films = num_films
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.films_embedding = layers.Embedding( # layer embeddings films
        num_films,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.films_bias = layers.Embedding(num_films, 1) # layer embedding films bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # layer embedding 2
    films_vector = self.films_embedding(inputs[:, 1]) # layer embedding 3
    films_bias = self.films_bias(inputs[:, 1]) # layer embedding 4

    dot_user_films = tf.tensordot(user_vector, films_vector, 2)

    x = dot_user_films + user_bias + films_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""compile model dilakukan menggunakan BinaryCrossentropy untuk menghitung loss function, Adam (Adaptive Moment Estimation) sebagai optimizer, dan root mean squared error (RMSE) sebagai metrics evaluation."""

model = RecommenderNet(num_users, num_films, 50) # model initialization

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""melakukan proses training dengan menentukan nilai epochs sebesar 25"""

# training
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 32,
    epochs = 25,
    validation_data = (x_val, y_val)
)

"""**Metric Visualization**"""

# plot metrics evaluations
plt.plot(history.history['root_mean_squared_error'], color='blue')
plt.plot(history.history['val_root_mean_squared_error'], color='red')
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""berdasarkan visualisasi, pada proses training model didapatkan nilai error akhir sebesar sekitar 0.17 dan error pada data validasi sebesar 0.26

**Get Recommendations**
"""

# films
films_df = df_film
films_df.head()

# data ratings
df = films
df.head()

"""mengambil sample data user secara acak dan mendefinisikan variabel 'film_not_visited' yang merupakan daftar film yang belum pernah ditonton oleh users"""

# taking user samples
user_id = df.userId.sample(1).iloc[0]
films_visited_by_user = df[df.userId == user_id]

# bitwise operators (~), can be found here https://docs.python.org/3/reference/expressions.html
films_not_visited = films_df[~films_df['film_id'].isin(films_visited_by_user.movieId.values)]['film_id']
films_not_visited = list(
    set(films_not_visited)
    .intersection(set(films_to_films_encoded.keys()))
)

films_not_visited = [[films_to_films_encoded.get(x)] for x in films_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_films_array = np.hstack(
    ([[user_encoder]] * len(films_not_visited), films_not_visited)
)

"""membuat rekomendasi film dengan model.predict()"""

# get recommendations
ratings = model.predict(user_films_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_films_ids = [
    films_encoded_to_films.get(films_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('films with high ratings from user')
print('----' * 8)

top_films_user = (
    films_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)

films_df_rows = films_df[films_df['film_id'].isin(top_films_user)]
for row in films_df_rows.itertuples():
    print(row.film_name, ':', row.genre)

print('----' * 8)
print('Top 10 films recommendation')
print('----' * 8)

recommended_films = films_df[films_df['film_id'].isin(recommended_films_ids)]
for row in recommended_films.itertuples():
    print(row.film_name, ':', row.genre)

"""Model sekarang dapat memberikan rekomendasi film kepada pengguna. Sebagai contoh, hasil rekomendasi untuk pengguna dengan ID 228 adalah film dengan genre 'Drama'. Informasi ini dapat diidentifikasi melalui rekomendasi dengan peringkat tinggi dari pengguna dan daftar 10 film teratas yang direkomendasikan untuk pengguna tersebut."""



"""# **Conclusion**

Pada proyek ini, saya telah berhasil mengembangkan sebuah sistem rekomendasi film dengan menggunakan dua teknik berbeda, yakni Content-Based Filtering dan Collaborative Filtering. Teknik collaborative filtering mampu memberikan rekomendasi yang sesuai dengan preferensi pengguna, sementara content-based filtering dapat merekomendasikan film yang memiliki kesamaan genre dengan film yang telah disukai. Perlu dicatat bahwa dataset yang digunakan memiliki jumlah data film yang terbatas, sehingga di dunia nyata model mungkin tidak memberikan rekomendasi sesuai keinginan pengguna, terutama untuk mereka yang menginginkan film-film terbaru. Model juga bisa merekomendasikan film lama karena keterbatasan data yang ada dalam dataset.
"""

